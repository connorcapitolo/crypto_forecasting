{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_Modeling_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wjOGLKWqk6Jh"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T63b4X2miwX"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv2iNx1EmuC-",
        "outputId": "7bf18efb-6d45-4db5-b5f0-c35991a9e73f"
      },
      "source": [
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
        "# without building graphs\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "print(\"tensorflow version\", tf.__version__)\n",
        "print(\"keras version\", tf.keras.__version__)\n",
        "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Get the number of replicas \n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "devices = tf.config.experimental.get_visible_devices()\n",
        "print(\"Devices:\", devices)\n",
        "print(tf.config.experimental.list_logical_devices('GPU'))\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/data_performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow version 2.7.0\n",
            "keras version 2.7.0\n",
            "Eager Execution Enabled: True\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of replicas: 1\n",
            "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYrHjWcbnAzP",
        "outputId": "81f1e9d7-43ce-4790-9c29-e6f36a0e9262"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lYhJActoAW1"
      },
      "source": [
        "COLUMNS:\n",
        "- Open Time: Candle Open Time\n",
        "- Open Price: Open Price in Quote (Secondary) Asset (USDT in the DF below)\n",
        "- High Price: High Price in Quote (Secondary) Asset (USDT in the DF below)\n",
        "- Low Price: Low Price in Quote (Secondary) Asset (USDT in the DF below)\n",
        "- Close Price: Close Price in Quote (Secondary) Asset (USDT in the DF below)\n",
        "- Volume Traded: Total Trade Volume in Primary (Base) Asset Units (BTC in the DF below)\n",
        "- Close Time: Candle Close Time\n",
        "- Quote Asset Volume: Total Trade Volume in Quote (Secondary) Asset Units (USDT in the DF below)\n",
        "- Number of Trades: Total Number of Trades\n",
        "- Taker Buy Base Asset Volume: Number of Trades in Primary (Base) Asset Resulting from a Taker Matching an Existing Order. In other words, it separates the 'Volume Traded Feature' by the amount traded as a result of orders baing placed in the order book and orders being matched in the order book (might need to revisit this, could provide additional interesting insights, if we have a better understanding of trading strategies). Volume Traded = Taker Buy Base Asset Volume + Maker Buy Base Asset Volume\n",
        "- Taker Buy Quote Asset Volume: Number of Trades in Quote (Secondary) Asset Resulting from a Taker Mathcing an Existing Order. Explanation similar to the above, only difference the volume is calculated in Quote (Secondary) Asset units. Quote Asset Volume = Takes Buy Quote Asser Volume + Maker Buy Quote Asset Volume\n",
        "- NA: Safe to Ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S72Qc4DKs6X5"
      },
      "source": [
        "# START HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/btcusdt_data.csv', index_col=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic9GD16TZoS0",
        "outputId": "f9be0c45-b35f-4cb6-f176-8d7cf2964345"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEMPLATE CODE"
      ],
      "metadata": {
        "id": "Jy8asGeOC_vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_folder = '../Data'\n",
        "file_name = 'btcusdt_exchange_full.csv'\n",
        "\n",
        "#location_data = os.path.join(data_folder, file_name)\n",
        "location_data = '/content/gdrive/MyDrive/playground/btcusdt_data.csv'\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "# log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "#  'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev', 'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "#  'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w', 'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "#  'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m', 'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "#  'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d', 'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h', 'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m', 'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "#  'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h', 'rolling_min_high_open_1d',\n",
        "#  'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h', 'rolling_avg_open_low_1d',\n",
        "#  'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h', 'rolling_max_open_low_1d',\n",
        "#  'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h', 'rolling_min_open_low_1d',\n",
        "#  'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h', 'rolling_avg_spread_1d',\n",
        "#  'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h', 'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "#  'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d', 'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "#  'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "#  'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w', 'rolling_max_Open Price_1m',\n",
        "#  'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w', 'rolling_min_Open Price_1m',\n",
        "#  'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w', 'rolling_avg_High price_1m',\n",
        "#  'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w', 'rolling_max_High price_1m',\n",
        "#  'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w', 'rolling_min_High price_1m',\n",
        "#  'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w', 'rolling_avg_Low Price_1m',\n",
        "#  'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w', 'rolling_max_Low Price_1m',\n",
        "#  'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w', 'rolling_min_Low Price_1m',\n",
        "#  'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w', 'rolling_avg_Close Price_1m',\n",
        "#  'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w', 'rolling_max_Close Price_1m',\n",
        "#  'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w', 'rolling_min_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "uErQFnCv1EtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    if train_val_date is None and val_test_date is None:\n",
        "        index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "        print(index_val, index_test)\n",
        "        print(df.head(), len(df.index))\n",
        "        train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df_train.columns if col not in exclude_x]\n",
        "    X_train, X_val, X_test = df_train[features_list], df_val[features_list],df_test[features_list]\n",
        "    y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, features_list\n",
        "\n",
        "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = X_train.std()\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std()\n",
        "    X_train, X_val, X_test = (X_train - x_mean) / x_std, (X_val - x_mean) / x_std, (X_test - x_mean) / x_std\n",
        "    y_train, y_val, y_test = (y_train - y_mean) / y_std, (y_val - y_mean) / y_std, (y_test - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, x_val, y_train, y_val, y_train_baseline, y_val_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "    y_val = tf.convert_to_tensor(y_val-y_val_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    validation_data = validation_data.batch(batch_size)\n",
        "    validation_data = validation_data.prefetch(AUTOTUNE)\n",
        "    return train_data, validation_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, features_list = _split(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std = standardize(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    X_val_seq = generate_input_sequences(X_val, X_val.columns.values.tolist())\n",
        "    X_test_seq = generate_input_sequences(X_test, X_test.columns.values.tolist())\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    y_val_seq = generate_output_sequences(y_val, y_val.columns.values.tolist())\n",
        "    y_test_seq = generate_output_sequences(y_test, y_test.columns.values.tolist())\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    y_val_baseline = generate_baseline_predictions(y_val, y_val.columns.values.tolist())\n",
        "    y_test_baseline = generate_baseline_predictions(y_test, y_test.columns.values.tolist())\n",
        "    print('Baseline done')\n",
        "    train_data, validation_data = generate_tf_data(X_train_seq, X_val_seq, y_train_seq, y_val_seq, y_train_baseline, y_val_baseline, batch_size)\n",
        "    return train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/btcusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model.add(BatchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "P_qSPiOK2hmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "rlL3y_-VP7qb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'crypto-forecasting-app'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ht212esQCPr",
        "outputId": "f4c26ed1-942b-462b-da2c-a35e7019db58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "gs://artifacts.crypto-forecasting-app.appspot.com/\n",
            "gs://crypto-forecasting-bucket/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL MODELING PER PAIR STARTS HERE"
      ],
      "metadata": {
        "id": "iJAUX1vuERXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BTCUSDT MODELING"
      ],
      "metadata": {
        "id": "wVfLrelJO0ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# data_folder = '../Data'\n",
        "# file_name = 'btcusdt_exchange_full.csv'\n",
        "\n",
        "#location_data = os.path.join(data_folder, file_name)\n",
        "location_data = '/content/gdrive/MyDrive/playground/btcusdt_data.csv'\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "4Uj1zErZSxBU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    if train_val_date is None and val_test_date is None:\n",
        "        index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "        print(index_val, index_test)\n",
        "        print(df.head(), len(df.index))\n",
        "        train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df_train.columns if col not in exclude_x]\n",
        "    X_train, X_val, X_test = df_train[features_list], df_val[features_list],df_test[features_list]\n",
        "    y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, features_list\n",
        "\n",
        "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = X_train.std()\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std()\n",
        "    X_train, X_val, X_test = (X_train - x_mean) / x_std, (X_val - x_mean) / x_std, (X_test - x_mean) / x_std\n",
        "    y_train, y_val, y_test = (y_train - y_mean) / y_std, (y_val - y_mean) / y_std, (y_test - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, x_val, y_train, y_val, y_train_baseline, y_val_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "    y_val = tf.convert_to_tensor(y_val-y_val_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    validation_data = validation_data.batch(batch_size)\n",
        "    validation_data = validation_data.prefetch(AUTOTUNE)\n",
        "    return train_data, validation_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, features_list = _split(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std = standardize(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    X_val_seq = generate_input_sequences(X_val, X_val.columns.values.tolist())\n",
        "    X_test_seq = generate_input_sequences(X_test, X_test.columns.values.tolist())\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    y_val_seq = generate_output_sequences(y_val, y_val.columns.values.tolist())\n",
        "    y_test_seq = generate_output_sequences(y_test, y_test.columns.values.tolist())\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    y_val_baseline = generate_baseline_predictions(y_val, y_val.columns.values.tolist())\n",
        "    y_test_baseline = generate_baseline_predictions(y_test, y_test.columns.values.tolist())\n",
        "    print('Baseline done')\n",
        "    train_data, validation_data = generate_tf_data(X_train_seq, X_val_seq, y_train_seq, y_val_seq, y_train_baseline, y_val_baseline, batch_size)\n",
        "    return train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/btcusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model1 = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model1.add(BatchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model1.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model1.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history1 = lstm_model1.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVXxf8wghFJG",
        "outputId": "88a675a8-1986-4ada-e383-ea17dd619f5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "759999 854999\n",
            "       Open Time  ...  log_rolling_max_Close Price_1m\n",
            "0  1582259340000  ...                        9.259226\n",
            "1  1582259400000  ...                        9.259226\n",
            "2  1582259460000  ...                        9.259226\n",
            "3  1582259520000  ...                        9.259226\n",
            "4  1582259580000  ...                        9.259226\n",
            "\n",
            "[5 rows x 107 columns] 949999\n",
            "We are going to build a model with train/val date 2021-08-02 23:09:00 and val/test date 2021-10-08 04:59:00\n",
            "Splits done\n",
            "Standardization done\n",
            "Inputs done\n",
            "Outputs done\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "5938/5938 [==============================] - 71s 11ms/step - loss: 5.3260e-04 - mean_absolute_error: 0.0058 - mean_squared_error: 2.7836e-04 - val_loss: 9.6257e-05 - val_mean_absolute_error: 0.0061 - val_mean_squared_error: 9.5889e-05\n",
            "Epoch 2/10\n",
            "5938/5938 [==============================] - 61s 10ms/step - loss: 7.6485e-05 - mean_absolute_error: 0.0041 - mean_squared_error: 7.5949e-05 - val_loss: 1.1867e-04 - val_mean_absolute_error: 0.0073 - val_mean_squared_error: 1.1841e-04\n",
            "Epoch 3/10\n",
            "5938/5938 [==============================] - 61s 10ms/step - loss: 6.9892e-05 - mean_absolute_error: 0.0040 - mean_squared_error: 6.8703e-05 - val_loss: 1.7577e-04 - val_mean_absolute_error: 0.0091 - val_mean_squared_error: 1.7115e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    # if train_val_date is None and val_test_date is None:\n",
        "    #     index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "    #     print(index_val, index_test)\n",
        "    #     print(df.head(), len(df.index))\n",
        "    #     train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    # print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    # df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    y = df[['y']]\n",
        "    print('Splits done')\n",
        "    return X, y, features_list\n",
        "\n",
        "def standardize(X, y):\n",
        "    x_mean = X.mean()\n",
        "    x_std = X.std()\n",
        "    y_mean = y.mean()\n",
        "    y_std = y.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    y = (y - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X, y, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, y_train, y_train_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "    return train_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, y_train, features_list = _split(df)\n",
        "    # display(X_train) WORKS\n",
        "    X_train, y_train, x_mean, x_std, y_mean, y_std = standardize(X_train, y_train)\n",
        "    # print(X_train.shape) WORKS\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    print(X_train_seq.shape)\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_seq.shape)\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_baseline.shape)\n",
        "    print('Baseline done')\n",
        "    train_data = generate_tf_data(X_train_seq, y_train_seq, y_train_baseline, batch_size)\n",
        "    return train_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/btcusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model.add(BatchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jly-Tgj4S52w",
        "outputId": "43b50460-899a-4f74-d766-99c2c45359c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "Splits done\n",
            "Standardization done\n",
            "(949960, 8, 102)\n",
            "Inputs done\n",
            "(949960, 32, 1)\n",
            "Outputs done\n",
            "(949960, 32, 1)\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 3.7299e-04 - mean_absolute_error: 0.0055 - mean_squared_error: 2.0299e-04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 79s 10ms/step - loss: 3.7299e-04 - mean_absolute_error: 0.0055 - mean_squared_error: 2.0299e-04\n",
            "Epoch 2/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 6.6503e-05 - mean_absolute_error: 0.0041 - mean_squared_error: 6.5916e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 6.6507e-05 - mean_absolute_error: 0.0041 - mean_squared_error: 6.5917e-05\n",
            "Epoch 3/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 5.6631e-05 - mean_absolute_error: 0.0038 - mean_squared_error: 5.5213e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 5.6633e-05 - mean_absolute_error: 0.0038 - mean_squared_error: 5.5213e-05\n",
            "Epoch 4/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 5.2334e-05 - mean_absolute_error: 0.0037 - mean_squared_error: 5.0794e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 5.2328e-05 - mean_absolute_error: 0.0037 - mean_squared_error: 5.0785e-05\n",
            "Epoch 5/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 5.0110e-05 - mean_absolute_error: 0.0036 - mean_squared_error: 4.8383e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 5.0106e-05 - mean_absolute_error: 0.0036 - mean_squared_error: 4.8378e-05\n",
            "Epoch 6/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 4.8189e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.6381e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.8185e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.6376e-05\n",
            "Epoch 7/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 4.7627e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.5836e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.7620e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.5827e-05\n",
            "Epoch 8/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 4.6458e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.4676e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.6456e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.4672e-05\n",
            "Epoch 9/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 4.5900e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.4038e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 4.5903e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 4.4038e-05\n",
            "Epoch 10/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 4.4918e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 4.2918e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 71s 10ms/step - loss: 4.4917e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 4.2915e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /content/gdrive/MyDrive/playground/LSTM_BTCUSDT_12-12-2021/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEUh70pbaW8m",
        "outputId": "e478394f-de0f-4cbb-8d0e-f4f04efdf377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/gdrive/MyDrive/playground/LSTM_BTCUSDT_12-12-2021/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = {}\n",
        "\n",
        "data['validation_score'] = history1.history['val_loss'][-1]\n",
        "data['unique_id'] = time.time()\n",
        "data['training_time_stamp'] = time.time()\n",
        "data['hyperparameters'] = {'batch_size': 128, 'lr': 1e-3, 'optimizer': 'Adam', 'n_layers': 3, 'n_units': 8, 'dropout': 0.2, 'weight_decay': 1e-4}\n",
        "data['symbol'] = 'BTCUSDT'\n",
        "data['x_mean'] = x_mean.values.tolist()\n",
        "data['x_std'] = x_std.values.tolist()\n",
        "data['y_mean'] = y_mean.values[0]\n",
        "data['y_std'] = y_std.values[0]\n",
        "\n",
        "with open('model_metrics_BTCUSDT.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "GskryTwm88l6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/gdrive/MyDrive/playground/LSTM_BTCUSDT_$(date +\"%d-%m-%Y\")"
      ],
      "metadata": {
        "id": "fWMZZaFLO5mB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/model_metrics_BTCUSDT.json /content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/"
      ],
      "metadata": {
        "id": "tM3FOqYwPFhD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save_weights('/content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/lstm_BTCUSDT')"
      ],
      "metadata": {
        "id": "Ia_MWsCBPFmc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/ gs://crypto-forecasting-bucket/BTCUSDT/Model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKhippEcPFoy",
        "outputId": "6be89a03-8369-44b7-cbff-eca663ce2a0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/model_metrics_BTCUSDT.json [Content-Type=application/json]...\n",
            "/ [0/4 files][    0.0 B/427.6 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/lstm_BTCUSDT.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/lstm_BTCUSDT.index [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/427.6 KiB]   0% Done                                    \r/ [0/4 files][    0.0 B/427.6 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BTCUSDT_13-12-2021/checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/427.6 KiB]   0% Done                                    \r/ [1/4 files][427.6 KiB/427.6 KiB]  99% Done                                    \r/ [2/4 files][427.6 KiB/427.6 KiB]  99% Done                                    \r/ [3/4 files][427.6 KiB/427.6 KiB]  99% Done                                    \r/ [4/4 files][427.6 KiB/427.6 KiB] 100% Done                                    \r\n",
            "Operation completed over 4 objects/427.6 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BNBBTC MODELING"
      ],
      "metadata": {
        "id": "rIWWty0TQM8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "location_data = '/content/gdrive/MyDrive/playground/bnbbtc_data.csv'\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "i19hIVDVZcy2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    if train_val_date is None and val_test_date is None:\n",
        "        index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "        print(index_val, index_test)\n",
        "        print(df.head(), len(df.index))\n",
        "        train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df_train.columns if col not in exclude_x]\n",
        "    X_train, X_val, X_test = df_train[features_list], df_val[features_list],df_test[features_list]\n",
        "    y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, features_list\n",
        "\n",
        "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = X_train.std()\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std()\n",
        "    X_train, X_val, X_test = (X_train - x_mean) / x_std, (X_val - x_mean) / x_std, (X_test - x_mean) / x_std\n",
        "    y_train, y_val, y_test = (y_train - y_mean) / y_std, (y_val - y_mean) / y_std, (y_test - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, x_val, y_train, y_val, y_train_baseline, y_val_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "    y_val = tf.convert_to_tensor(y_val-y_val_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    validation_data = validation_data.batch(batch_size)\n",
        "    validation_data = validation_data.prefetch(AUTOTUNE)\n",
        "    return train_data, validation_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, features_list = _split(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std = standardize(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    X_val_seq = generate_input_sequences(X_val, X_val.columns.values.tolist())\n",
        "    X_test_seq = generate_input_sequences(X_test, X_test.columns.values.tolist())\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    y_val_seq = generate_output_sequences(y_val, y_val.columns.values.tolist())\n",
        "    y_test_seq = generate_output_sequences(y_test, y_test.columns.values.tolist())\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    y_val_baseline = generate_baseline_predictions(y_val, y_val.columns.values.tolist())\n",
        "    y_test_baseline = generate_baseline_predictions(y_test, y_test.columns.values.tolist())\n",
        "    print('Baseline done')\n",
        "    train_data, validation_data = generate_tf_data(X_train_seq, X_val_seq, y_train_seq, y_val_seq, y_train_baseline, y_val_baseline, batch_size)\n",
        "    return train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/bnbbtc_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model1 = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model1.add(BatchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model1.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model1.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history1 = lstm_model1.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZllpUckh7ig",
        "outputId": "2c13b175-1932-4d6d-c850-7d333c46f3e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "759999 854999\n",
            "       Open Time  ...  log_rolling_max_Close Price_1m\n",
            "0  1582257540000  ...                        0.002614\n",
            "1  1582257600000  ...                        0.002614\n",
            "2  1582257660000  ...                        0.002614\n",
            "3  1582257720000  ...                        0.002614\n",
            "4  1582257780000  ...                        0.002614\n",
            "\n",
            "[5 rows x 107 columns] 949999\n",
            "We are going to build a model with train/val date 2021-08-02 22:39:00 and val/test date 2021-10-08 04:29:00\n",
            "Splits done\n",
            "Standardization done\n",
            "Inputs done\n",
            "Outputs done\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "5938/5938 [==============================] - 69s 10ms/step - loss: 4.7369e-04 - mean_absolute_error: 0.0051 - mean_squared_error: 2.4963e-04 - val_loss: 8.0415e-05 - val_mean_absolute_error: 0.0059 - val_mean_squared_error: 8.0224e-05\n",
            "Epoch 2/10\n",
            "5938/5938 [==============================] - 62s 10ms/step - loss: 6.5176e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 6.4835e-05 - val_loss: 7.9304e-05 - val_mean_absolute_error: 0.0059 - val_mean_squared_error: 7.9215e-05\n",
            "Epoch 3/10\n",
            "5938/5938 [==============================] - 61s 10ms/step - loss: 6.1890e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 6.1277e-05 - val_loss: 1.9603e-04 - val_mean_absolute_error: 0.0101 - val_mean_squared_error: 1.9513e-04\n",
            "Epoch 4/10\n",
            "5938/5938 [==============================] - 63s 11ms/step - loss: 5.9585e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.8778e-05 - val_loss: 9.7453e-05 - val_mean_absolute_error: 0.0068 - val_mean_squared_error: 9.5689e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    y = df[['y']]\n",
        "    print('Splits done')\n",
        "    return X, y, features_list\n",
        "\n",
        "def standardize(X, y):\n",
        "    x_mean = X.mean()\n",
        "    x_std = X.std()\n",
        "    y_mean = y.mean()\n",
        "    y_std = y.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    y = (y - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X, y, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, y_train, y_train_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "    return train_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, y_train, features_list = _split(df)\n",
        "    # display(X_train) WORKS\n",
        "    X_train, y_train, x_mean, x_std, y_mean, y_std = standardize(X_train, y_train)\n",
        "    # print(X_train.shape) WORKS\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    print(X_train_seq.shape)\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_seq.shape)\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_baseline.shape)\n",
        "    print('Baseline done')\n",
        "    train_data = generate_tf_data(X_train_seq, y_train_seq, y_train_baseline, batch_size)\n",
        "    return train_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/bnbbtc_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model.add(BatchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRoCbQCCZfi4",
        "outputId": "6371393d-26ca-4b26-ce83-dc5dcaeb0b78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "Splits done\n",
            "Standardization done\n",
            "(949960, 8, 102)\n",
            "Inputs done\n",
            "(949960, 32, 1)\n",
            "Outputs done\n",
            "(949960, 32, 1)\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 3.6203e-04 - mean_absolute_error: 0.0046 - mean_squared_error: 1.8313e-04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 79s 10ms/step - loss: 3.6203e-04 - mean_absolute_error: 0.0046 - mean_squared_error: 1.8313e-04\n",
            "Epoch 2/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 5.2998e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.2644e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 5.2998e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.2644e-05\n",
            "Epoch 3/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 5.0858e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.0273e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 5.0854e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.0267e-05\n",
            "Epoch 4/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 4.8001e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 4.7254e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 4.8001e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 4.7254e-05\n",
            "Epoch 5/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 4.6952e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.6119e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.6942e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.6107e-05\n",
            "Epoch 6/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 4.4854e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.3881e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.4846e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.3870e-05\n",
            "Epoch 7/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 4.4475e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.3435e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.4457e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.3415e-05\n",
            "Epoch 8/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 4.3833e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.2799e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.3826e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.2790e-05\n",
            "Epoch 9/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 4.4220e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.3193e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 4.4212e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.3184e-05\n",
            "Epoch 10/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 4.2551e-05 - mean_absolute_error: 0.0030 - mean_squared_error: 4.1416e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 71s 10ms/step - loss: 4.2541e-05 - mean_absolute_error: 0.0030 - mean_squared_error: 4.1403e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /gdrive/MyDrive/playground/LSTM_BNBBTC_12-12-2021/"
      ],
      "metadata": {
        "id": "maqNKBhsQ76N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = {}\n",
        "\n",
        "data['validation_score'] = history1.history['val_loss'][-1]\n",
        "data['unique_id'] = time.time()\n",
        "data['training_time_stamp'] = time.time()\n",
        "data['hyperparameters'] = {'batch_size': 128, 'lr': 1e-3, 'optimizer': 'Adam', 'n_layers': 3, 'n_units': 8, 'dropout': 0.2, 'weight_decay': 1e-4}\n",
        "data['symbol'] = 'BNBBTC'\n",
        "data['x_mean'] = x_mean.values.tolist()\n",
        "data['x_std'] = x_std.values.tolist()\n",
        "data['y_mean'] = y_mean.values[0]\n",
        "data['y_std'] = y_std.values[0]\n",
        "\n",
        "with open('model_metrics_BNBBTC.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "vYLcb5sIQdRo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/gdrive/MyDrive/playground/LSTM_BNBBTC_$(date +\"%d-%m-%Y\")"
      ],
      "metadata": {
        "id": "ycz04O2ZQdWx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/model_metrics_BNBBTC.json /content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/"
      ],
      "metadata": {
        "id": "bdHSPLEzQdcq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save_weights('/content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/lstm_BNBBTC')"
      ],
      "metadata": {
        "id": "gMP0GV8oQdoG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/ gs://crypto-forecasting-bucket/BNBBTC/Model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko2G1w3cQuD3",
        "outputId": "426be070-3e22-41c8-8fec-81382f04c876"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/model_metrics_BNBBTC.json [Content-Type=application/json]...\n",
            "/ [0/4 files][    0.0 B/428.2 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/lstm_BNBBTC.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/428.2 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/lstm_BNBBTC.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_BNBBTC_13-12-2021/checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [4/4 files][428.2 KiB/428.2 KiB] 100% Done                                    \n",
            "Operation completed over 4 objects/428.2 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BNBUSDT MODELING"
      ],
      "metadata": {
        "id": "-gbM6YGJROPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "location_data = '/content/gdrive/MyDrive/playground/bnbusdt_data.csv'\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "u7ECsqKbaLCs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    if train_val_date is None and val_test_date is None:\n",
        "        index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "        print(index_val, index_test)\n",
        "        print(df.head(), len(df.index))\n",
        "        train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df_train.columns if col not in exclude_x]\n",
        "    X_train, X_val, X_test = df_train[features_list], df_val[features_list],df_test[features_list]\n",
        "    y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, features_list\n",
        "\n",
        "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = X_train.std()\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std()\n",
        "    X_train, X_val, X_test = (X_train - x_mean) / x_std, (X_val - x_mean) / x_std, (X_test - x_mean) / x_std\n",
        "    y_train, y_val, y_test = (y_train - y_mean) / y_std, (y_val - y_mean) / y_std, (y_test - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, x_val, y_train, y_val, y_train_baseline, y_val_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "    y_val = tf.convert_to_tensor(y_val-y_val_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    validation_data = validation_data.batch(batch_size)\n",
        "    validation_data = validation_data.prefetch(AUTOTUNE)\n",
        "    return train_data, validation_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, features_list = _split(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std = standardize(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    X_val_seq = generate_input_sequences(X_val, X_val.columns.values.tolist())\n",
        "    X_test_seq = generate_input_sequences(X_test, X_test.columns.values.tolist())\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    y_val_seq = generate_output_sequences(y_val, y_val.columns.values.tolist())\n",
        "    y_test_seq = generate_output_sequences(y_test, y_test.columns.values.tolist())\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    y_val_baseline = generate_baseline_predictions(y_val, y_val.columns.values.tolist())\n",
        "    y_test_baseline = generate_baseline_predictions(y_test, y_test.columns.values.tolist())\n",
        "    print('Baseline done')\n",
        "    train_data, validation_data = generate_tf_data(X_train_seq, X_val_seq, y_train_seq, y_val_seq, y_train_baseline, y_val_baseline, batch_size)\n",
        "    return train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/bnbusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model1 = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model1.add(BatchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model1.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model1.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history1 = lstm_model1.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThyTnq2Qm0LX",
        "outputId": "eb07e977-49de-43b1-f543-faf10d2250b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "759999 854999\n",
            "       Open Time  ...  log_rolling_max_Close Price_1m\n",
            "0  1582261200000  ...                        3.338463\n",
            "1  1582261260000  ...                        3.338463\n",
            "2  1582261320000  ...                        3.338463\n",
            "3  1582261380000  ...                        3.338463\n",
            "4  1582261440000  ...                        3.338463\n",
            "\n",
            "[5 rows x 107 columns] 949999\n",
            "We are going to build a model with train/val date 2021-08-02 23:41:00 and val/test date 2021-10-08 05:31:00\n",
            "Splits done\n",
            "Standardization done\n",
            "Inputs done\n",
            "Outputs done\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "5938/5938 [==============================] - 55s 8ms/step - loss: 5.0862e-04 - mean_absolute_error: 0.0055 - mean_squared_error: 2.8590e-04 - val_loss: 1.3513e-04 - val_mean_absolute_error: 0.0074 - val_mean_squared_error: 1.3494e-04\n",
            "Epoch 2/10\n",
            "5938/5938 [==============================] - 45s 8ms/step - loss: 1.0972e-04 - mean_absolute_error: 0.0038 - mean_squared_error: 1.0930e-04 - val_loss: 1.4219e-04 - val_mean_absolute_error: 0.0076 - val_mean_squared_error: 1.4186e-04\n",
            "Epoch 3/10\n",
            "5938/5938 [==============================] - 45s 8ms/step - loss: 1.0275e-04 - mean_absolute_error: 0.0036 - mean_squared_error: 1.0194e-04 - val_loss: 2.8302e-04 - val_mean_absolute_error: 0.0122 - val_mean_squared_error: 2.8257e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    y = df[['y']]\n",
        "    print('Splits done')\n",
        "    return X, y, features_list\n",
        "\n",
        "def standardize(X, y):\n",
        "    x_mean = X.mean()\n",
        "    x_std = X.std()\n",
        "    y_mean = y.mean()\n",
        "    y_std = y.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    y = (y - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X, y, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, y_train, y_train_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "    return train_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, y_train, features_list = _split(df)\n",
        "    # display(X_train) WORKS\n",
        "    X_train, y_train, x_mean, x_std, y_mean, y_std = standardize(X_train, y_train)\n",
        "    # print(X_train.shape) WORKS\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    print(X_train_seq.shape)\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_seq.shape)\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_baseline.shape)\n",
        "    print('Baseline done')\n",
        "    train_data = generate_tf_data(X_train_seq, y_train_seq, y_train_baseline, batch_size)\n",
        "    return train_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/bnbusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model.add(BatchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvB20FOUaRYr",
        "outputId": "dd497dd4-6ca4-461a-c729-d3b0f9031983"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "Splits done\n",
            "Standardization done\n",
            "(949960, 8, 102)\n",
            "Inputs done\n",
            "(949960, 32, 1)\n",
            "Outputs done\n",
            "(949960, 32, 1)\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 3.8807e-04 - mean_absolute_error: 0.0051 - mean_squared_error: 2.1154e-04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 60s 7ms/step - loss: 3.8799e-04 - mean_absolute_error: 0.0051 - mean_squared_error: 2.1149e-04\n",
            "Epoch 2/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 7.9617e-05 - mean_absolute_error: 0.0038 - mean_squared_error: 7.9001e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 56s 8ms/step - loss: 7.9611e-05 - mean_absolute_error: 0.0038 - mean_squared_error: 7.8992e-05\n",
            "Epoch 3/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 6.9721e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 6.8790e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 56s 8ms/step - loss: 6.9713e-05 - mean_absolute_error: 0.0035 - mean_squared_error: 6.8779e-05\n",
            "Epoch 4/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 6.2779e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 6.1584e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 57s 8ms/step - loss: 6.2776e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 6.1579e-05\n",
            "Epoch 5/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 6.0324e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 5.8893e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 56s 8ms/step - loss: 6.0324e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 5.8893e-05\n",
            "Epoch 6/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 5.8122e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 5.6636e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 58s 8ms/step - loss: 5.8115e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 5.6625e-05\n",
            "Epoch 7/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 5.7056e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 5.5333e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 56s 7ms/step - loss: 5.7048e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 5.5322e-05\n",
            "Epoch 8/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 5.6229e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 5.4280e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 56s 8ms/step - loss: 5.6226e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 5.4275e-05\n",
            "Epoch 9/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 5.5743e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 5.3816e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 57s 8ms/step - loss: 5.5740e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 5.3811e-05\n",
            "Epoch 10/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 5.4725e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 5.2673e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 57s 8ms/step - loss: 5.4723e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 5.2669e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /gdrive/MyDrive/playground/LSTM_BNBUSDT_12-12-2021/"
      ],
      "metadata": {
        "id": "LsXaRc14adBe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = {}\n",
        "\n",
        "data['validation_score'] = history1.history['val_loss'][-1]\n",
        "data['unique_id'] = time.time()\n",
        "data['training_time_stamp'] = time.time()\n",
        "data['hyperparameters'] = {'batch_size': 128, 'lr': 1e-3, 'optimizer': 'Adam', 'n_layers': 3, 'n_units': 8, 'dropout': 0.2, 'weight_decay': 1e-4}\n",
        "data['symbol'] = 'BNBUSDT'\n",
        "data['x_mean'] = x_mean.values.tolist()\n",
        "data['x_std'] = x_std.values.tolist()\n",
        "data['y_mean'] = y_mean.values[0]\n",
        "data['y_std'] = y_std.values[0]\n",
        "\n",
        "with open('model_metrics_BNBUSDT.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "-y5V1S0vQuJB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/gdrive/MyDrive/playground/LSTM_BNBUSDT_$(date +\"%d-%m-%Y\")"
      ],
      "metadata": {
        "id": "wyHaOWdhRcb8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/model_metrics_BNBUSDT.json /content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/"
      ],
      "metadata": {
        "id": "todTjMj0Rcfk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save_weights('/content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/lstm_BNBUSDT')"
      ],
      "metadata": {
        "id": "TFwuaWvSRciB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/ gs://crypto-forecasting-bucket/BNBUSDT/Model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYvxvn6WRcnF",
        "outputId": "ebc67b8a-a988-41ed-9726-8a0bf6927b55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/model_metrics_BNBUSDT.json [Content-Type=application/json]...\n",
            "/ [0/4 files][    0.0 B/427.7 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/lstm_BNBUSDT.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/427.7 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/lstm_BNBUSDT.index [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/427.7 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_BNBUSDT_13-12-2021/checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/427.7 KiB]   0% Done                                    \r/ [1/4 files][427.7 KiB/427.7 KiB]  99% Done                                    \r/ [2/4 files][427.7 KiB/427.7 KiB]  99% Done                                    \r/ [3/4 files][427.7 KiB/427.7 KiB]  99% Done                                    \r/ [4/4 files][427.7 KiB/427.7 KiB] 100% Done                                    \r\n",
            "Operation completed over 4 objects/427.7 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETHBTC MODELING"
      ],
      "metadata": {
        "id": "dGhexJ2eR-Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "location_data = '/content/gdrive/MyDrive/playground/ethbtc_data.csv'\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "4MmAZQXwaN97"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    if train_val_date is None and val_test_date is None:\n",
        "        index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "        print(index_val, index_test)\n",
        "        print(df.head(), len(df.index))\n",
        "        train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df_train.columns if col not in exclude_x]\n",
        "    X_train, X_val, X_test = df_train[features_list], df_val[features_list],df_test[features_list]\n",
        "    y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, features_list\n",
        "\n",
        "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = X_train.std()\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std()\n",
        "    X_train, X_val, X_test = (X_train - x_mean) / x_std, (X_val - x_mean) / x_std, (X_test - x_mean) / x_std\n",
        "    y_train, y_val, y_test = (y_train - y_mean) / y_std, (y_val - y_mean) / y_std, (y_test - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, x_val, y_train, y_val, y_train_baseline, y_val_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "    y_val = tf.convert_to_tensor(y_val-y_val_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    validation_data = validation_data.batch(batch_size)\n",
        "    validation_data = validation_data.prefetch(AUTOTUNE)\n",
        "    return train_data, validation_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, features_list = _split(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std = standardize(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    X_val_seq = generate_input_sequences(X_val, X_val.columns.values.tolist())\n",
        "    X_test_seq = generate_input_sequences(X_test, X_test.columns.values.tolist())\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    y_val_seq = generate_output_sequences(y_val, y_val.columns.values.tolist())\n",
        "    y_test_seq = generate_output_sequences(y_test, y_test.columns.values.tolist())\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    y_val_baseline = generate_baseline_predictions(y_val, y_val.columns.values.tolist())\n",
        "    y_test_baseline = generate_baseline_predictions(y_test, y_test.columns.values.tolist())\n",
        "    print('Baseline done')\n",
        "    train_data, validation_data = generate_tf_data(X_train_seq, X_val_seq, y_train_seq, y_val_seq, y_train_baseline, y_val_baseline, batch_size)\n",
        "    return train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/ethbtc_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model1 = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model1.add(BatchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model1.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model1.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history1 = lstm_model1.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojaPvYj0nJwH",
        "outputId": "3ccf45c5-cf9f-41d7-bf90-e4f6afe743bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "759999 854999\n",
            "       Open Time  ...  log_rolling_max_Close Price_1m\n",
            "0  1582264620000  ...                        0.028097\n",
            "1  1582264680000  ...                        0.028097\n",
            "2  1582264740000  ...                        0.028097\n",
            "3  1582264800000  ...                        0.028097\n",
            "4  1582264860000  ...                        0.028097\n",
            "\n",
            "[5 rows x 107 columns] 949999\n",
            "We are going to build a model with train/val date 2021-08-03 00:37:00 and val/test date 2021-10-08 06:27:00\n",
            "Splits done\n",
            "Standardization done\n",
            "Inputs done\n",
            "Outputs done\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "5938/5938 [==============================] - 70s 11ms/step - loss: 5.2227e-04 - mean_absolute_error: 0.0066 - mean_squared_error: 2.9095e-04 - val_loss: 1.6098e-04 - val_mean_absolute_error: 0.0087 - val_mean_squared_error: 1.6081e-04\n",
            "Epoch 2/10\n",
            "5938/5938 [==============================] - 62s 10ms/step - loss: 9.3987e-05 - mean_absolute_error: 0.0050 - mean_squared_error: 9.3394e-05 - val_loss: 1.4640e-04 - val_mean_absolute_error: 0.0079 - val_mean_squared_error: 1.4611e-04\n",
            "Epoch 3/10\n",
            "5938/5938 [==============================] - 62s 10ms/step - loss: 9.0772e-05 - mean_absolute_error: 0.0049 - mean_squared_error: 9.0010e-05 - val_loss: 1.7973e-04 - val_mean_absolute_error: 0.0091 - val_mean_squared_error: 1.7909e-04\n",
            "Epoch 4/10\n",
            "5938/5938 [==============================] - 63s 11ms/step - loss: 8.5896e-05 - mean_absolute_error: 0.0048 - mean_squared_error: 8.4618e-05 - val_loss: 2.2930e-04 - val_mean_absolute_error: 0.0106 - val_mean_squared_error: 2.2856e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    y = df[['y']]\n",
        "    print('Splits done')\n",
        "    return X, y, features_list\n",
        "\n",
        "def standardize(X, y):\n",
        "    x_mean = X.mean()\n",
        "    x_std = X.std()\n",
        "    y_mean = y.mean()\n",
        "    y_std = y.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    y = (y - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X, y, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, y_train, y_train_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "    return train_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, y_train, features_list = _split(df)\n",
        "    # display(X_train) WORKS\n",
        "    X_train, y_train, x_mean, x_std, y_mean, y_std = standardize(X_train, y_train)\n",
        "    # print(X_train.shape) WORKS\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    print(X_train_seq.shape)\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_seq.shape)\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_baseline.shape)\n",
        "    print('Baseline done')\n",
        "    train_data = generate_tf_data(X_train_seq, y_train_seq, y_train_baseline, batch_size)\n",
        "    return train_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/ethbtc_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model.add(BatchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4N1S1w8aSxl",
        "outputId": "c51ec2d1-095f-418b-f3df-08322ff52da8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "Splits done\n",
            "Standardization done\n",
            "(949960, 8, 102)\n",
            "Inputs done\n",
            "(949960, 32, 1)\n",
            "Outputs done\n",
            "(949960, 32, 1)\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 4.1454e-04 - mean_absolute_error: 0.0054 - mean_squared_error: 2.1317e-04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 77s 9ms/step - loss: 4.1451e-04 - mean_absolute_error: 0.0054 - mean_squared_error: 2.1314e-04\n",
            "Epoch 2/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 5.8217e-05 - mean_absolute_error: 0.0042 - mean_squared_error: 5.8015e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 70s 9ms/step - loss: 5.8224e-05 - mean_absolute_error: 0.0042 - mean_squared_error: 5.8020e-05\n",
            "Epoch 3/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 5.8312e-05 - mean_absolute_error: 0.0042 - mean_squared_error: 5.8022e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 71s 10ms/step - loss: 5.8319e-05 - mean_absolute_error: 0.0042 - mean_squared_error: 5.8027e-05\n",
            "Epoch 4/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 5.8538e-05 - mean_absolute_error: 0.0042 - mean_squared_error: 5.8254e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 71s 10ms/step - loss: 5.8536e-05 - mean_absolute_error: 0.0042 - mean_squared_error: 5.8249e-05\n",
            "Epoch 5/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 5.6529e-05 - mean_absolute_error: 0.0041 - mean_squared_error: 5.6049e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 70s 9ms/step - loss: 5.6539e-05 - mean_absolute_error: 0.0041 - mean_squared_error: 5.6057e-05\n",
            "Epoch 6/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 5.3581e-05 - mean_absolute_error: 0.0040 - mean_squared_error: 5.3115e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 5.3585e-05 - mean_absolute_error: 0.0040 - mean_squared_error: 5.3118e-05\n",
            "Epoch 7/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 5.1431e-05 - mean_absolute_error: 0.0039 - mean_squared_error: 5.0799e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 5.1431e-05 - mean_absolute_error: 0.0039 - mean_squared_error: 5.0799e-05\n",
            "Epoch 8/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 4.8237e-05 - mean_absolute_error: 0.0038 - mean_squared_error: 4.7470e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 4.8237e-05 - mean_absolute_error: 0.0038 - mean_squared_error: 4.7470e-05\n",
            "Epoch 9/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 4.7288e-05 - mean_absolute_error: 0.0037 - mean_squared_error: 4.6493e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 4.7295e-05 - mean_absolute_error: 0.0037 - mean_squared_error: 4.6498e-05\n",
            "Epoch 10/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 4.4444e-05 - mean_absolute_error: 0.0036 - mean_squared_error: 4.3502e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 72s 10ms/step - loss: 4.4450e-05 - mean_absolute_error: 0.0036 - mean_squared_error: 4.3506e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /gdrive/MyDrive/playground/LSTM_ETHBTC_12-12-2021/"
      ],
      "metadata": {
        "id": "YoZcKRERafwz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = {}\n",
        "\n",
        "data['validation_score'] = history1.history['val_loss'][-1]\n",
        "data['unique_id'] = time.time()\n",
        "data['training_time_stamp'] = time.time()\n",
        "data['hyperparameters'] = {'batch_size': 128, 'lr': 1e-3, 'optimizer': 'Adam', 'n_layers': 3, 'n_units': 8, 'dropout': 0.2, 'weight_decay': 1e-4}\n",
        "data['symbol'] = 'ETHBTC'\n",
        "data['x_mean'] = x_mean.values.tolist()\n",
        "data['x_std'] = x_std.values.tolist()\n",
        "data['y_mean'] = y_mean.values[0]\n",
        "data['y_std'] = y_std.values[0]\n",
        "\n",
        "with open('model_metrics_ETHBTC.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "IgCrCIB3QuK6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/gdrive/MyDrive/playground/LSTM_ETHBTC_$(date +\"%d-%m-%Y\")"
      ],
      "metadata": {
        "id": "i9odG943SCCZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/model_metrics_ETHBTC.json /content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/"
      ],
      "metadata": {
        "id": "dvGk2AQdSCHm"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save_weights('/content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/lstm_ETHBTC')"
      ],
      "metadata": {
        "id": "zyoOTL_cSCJ3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/ gs://crypto-forecasting-bucket/ETHBTC/Model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGCLdIgvSCL3",
        "outputId": "6a1369ac-7753-41d2-cd1e-b7df55255e06"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/lstm_ETHBTC.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/428.1 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/model_metrics_ETHBTC.json [Content-Type=application/json]...\n",
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/lstm_ETHBTC.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_ETHBTC_13-12-2021/checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [4/4 files][428.1 KiB/428.1 KiB] 100% Done                                    \n",
            "Operation completed over 4 objects/428.1 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETHUSDT MODELING"
      ],
      "metadata": {
        "id": "3rfVYJ1nSn4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "location_data = '/content/gdrive/MyDrive/playground/ethusdt_data.csv'\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "KyNeiglgaPIC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    if train_val_date is None and val_test_date is None:\n",
        "        index_val, index_test = int(len(df)*0.8), int(len(df)*0.9)\n",
        "        print(index_val, index_test)\n",
        "        print(df.head(), len(df.index))\n",
        "        train_val_date, val_test_date = df.loc[index_val, 'Date'], df.loc[index_test, 'Date']\n",
        "    print('We are going to build a model with train/val date {} and val/test date {}'.format(train_val_date, val_test_date))\n",
        "    df_train, df_val, df_test = df[df.Date <= pd.to_datetime(train_val_date)], df[(df.Date > pd.to_datetime(train_val_date)) & (df.Date <= pd.to_datetime(val_test_date))], df[df.Date > pd.to_datetime(val_test_date)]\n",
        "    features_list = [col for col in df_train.columns if col not in exclude_x]\n",
        "    X_train, X_val, X_test = df_train[features_list], df_val[features_list],df_test[features_list]\n",
        "    y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, features_list\n",
        "\n",
        "def standardize(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    x_mean = X_train.mean()\n",
        "    x_std = X_train.std()\n",
        "    y_mean = y_train.mean()\n",
        "    y_std = y_train.std()\n",
        "    X_train, X_val, X_test = (X_train - x_mean) / x_std, (X_val - x_mean) / x_std, (X_test - x_mean) / x_std\n",
        "    y_train, y_val, y_test = (y_train - y_mean) / y_std, (y_val - y_mean) / y_std, (y_test - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, x_val, y_train, y_val, y_train_baseline, y_val_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "    y_val = tf.convert_to_tensor(y_val-y_val_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "    validation_data = validation_data.batch(batch_size)\n",
        "    validation_data = validation_data.prefetch(AUTOTUNE)\n",
        "    return train_data, validation_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, features_list = _split(df)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, x_mean, x_std, y_mean, y_std = standardize(X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    X_val_seq = generate_input_sequences(X_val, X_val.columns.values.tolist())\n",
        "    X_test_seq = generate_input_sequences(X_test, X_test.columns.values.tolist())\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    y_val_seq = generate_output_sequences(y_val, y_val.columns.values.tolist())\n",
        "    y_test_seq = generate_output_sequences(y_test, y_test.columns.values.tolist())\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    y_val_baseline = generate_baseline_predictions(y_val, y_val.columns.values.tolist())\n",
        "    y_test_baseline = generate_baseline_predictions(y_test, y_test.columns.values.tolist())\n",
        "    print('Baseline done')\n",
        "    train_data, validation_data = generate_tf_data(X_train_seq, X_val_seq, y_train_seq, y_val_seq, y_train_baseline, y_val_baseline, batch_size)\n",
        "    return train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/ethusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, validation_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model1 = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model1.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model1.add(BatchNormalization())\n",
        "            lstm_model1.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model1.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model1.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history1 = lstm_model1.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPz0NzW-nYMG",
        "outputId": "a6f8e08f-eb34-4704-96e8-5a47f288ec0e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "759999 854999\n",
            "       Open Time  ...  log_rolling_max_Close Price_1m\n",
            "0  1582298460000  ...                        5.666946\n",
            "1  1582298520000  ...                        5.666946\n",
            "2  1582298580000  ...                        5.666946\n",
            "3  1582298640000  ...                        5.666946\n",
            "4  1582298700000  ...                        5.666946\n",
            "\n",
            "[5 rows x 107 columns] 949999\n",
            "We are going to build a model with train/val date 2021-08-03 10:02:00 and val/test date 2021-10-08 15:52:00\n",
            "Splits done\n",
            "Standardization done\n",
            "Inputs done\n",
            "Outputs done\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "5938/5938 [==============================] - 71s 11ms/step - loss: 4.9728e-04 - mean_absolute_error: 0.0059 - mean_squared_error: 2.7411e-04 - val_loss: 2.6904e-04 - val_mean_absolute_error: 0.0105 - val_mean_squared_error: 2.6888e-04\n",
            "Epoch 2/10\n",
            "5938/5938 [==============================] - 63s 11ms/step - loss: 1.0516e-04 - mean_absolute_error: 0.0042 - mean_squared_error: 1.0453e-04 - val_loss: 6.4427e-04 - val_mean_absolute_error: 0.0185 - val_mean_squared_error: 6.4287e-04\n",
            "Epoch 3/10\n",
            "5938/5938 [==============================] - 62s 10ms/step - loss: 9.8643e-05 - mean_absolute_error: 0.0041 - mean_squared_error: 9.7773e-05 - val_loss: 5.9346e-04 - val_mean_absolute_error: 0.0178 - val_mean_squared_error: 5.9207e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def select_last_year(df):\n",
        "    df = df.sort_values('Open Time')\n",
        "    df = df.iloc[-1000000:, :]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print('Only Selecting the last 2 years of data')\n",
        "    return df \n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    y = df[['y']]\n",
        "    print('Splits done')\n",
        "    return X, y, features_list\n",
        "\n",
        "def standardize(X, y):\n",
        "    x_mean = X.mean()\n",
        "    x_std = X.std()\n",
        "    y_mean = y.mean()\n",
        "    y_std = y.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    y = (y - y_mean) / y_std\n",
        "    print('Standardization done')\n",
        "    return X, y, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "def generate_input_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, input_seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in (range(input_seq_len, len(data)-output_seq_len+1)):\n",
        "        x = np.expand_dims(dfvs[i-input_seq_len:i, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_output_sequences(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(dfvs[i:i+output_seq_len, :], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_baseline_predictions(data, feats, input_seq_len = 8, output_seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-input_seq_len-output_seq_len+1, output_seq_len, 1))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(input_seq_len, len(data)-output_seq_len+1):\n",
        "        x = np.expand_dims(np.ones((output_seq_len, 1))*dfvs[i][0], axis = 0)\n",
        "        empty_np_array[i-input_seq_len] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(x_train, y_train, y_train_baseline, batch_size):\n",
        "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "    y_train = tf.convert_to_tensor(y_train-y_train_baseline, dtype=tf.float32)\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_data = train_data.batch(batch_size)\n",
        "    train_data = train_data.prefetch(AUTOTUNE)\n",
        "    return train_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    df = log_transformation(df)\n",
        "    X_train, y_train, features_list = _split(df)\n",
        "    # display(X_train) WORKS\n",
        "    X_train, y_train, x_mean, x_std, y_mean, y_std = standardize(X_train, y_train)\n",
        "    # print(X_train.shape) WORKS\n",
        "    X_train_seq = generate_input_sequences(X_train, X_train.columns.values.tolist())\n",
        "    print(X_train_seq.shape)\n",
        "    print('Inputs done')\n",
        "    y_train_seq = generate_output_sequences(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_seq.shape)\n",
        "    print('Outputs done')\n",
        "    y_train_baseline = generate_baseline_predictions(y_train, y_train.columns.values.tolist())\n",
        "    print(y_train_baseline.shape)\n",
        "    print('Baseline done')\n",
        "    train_data = generate_tf_data(X_train_seq, y_train_seq, y_train_baseline, batch_size)\n",
        "    return train_data, features_list, x_mean, x_std, y_mean, y_std\n",
        "\n",
        "print('here')\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/playground/ethusdt_data.csv', index_col=0)\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "optimizer = 'Adam'\n",
        "n_layers = 3\n",
        "n_units = 32\n",
        "dropout = 0.2\n",
        "weight_decay = 1e-4\n",
        "train_data, features_list, x_mean, x_std, y_mean, y_std = preprocess_df(df, batch_size)\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "elif optimizer == 'SGD':\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "if n_layers == 1:\n",
        "    lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "else:\n",
        "    for n in range(n_layers):\n",
        "        if n == n_layers - 1:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "            #lstm_model.add(batchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "        else:\n",
        "            lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "            lstm_model.add(BatchNormalization())\n",
        "            lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            \n",
        "lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "# history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, validation_data=validation_data, callbacks=[early_stopping, WandbCallback()])\n",
        "history = lstm_model.fit(train_data, epochs=MAX_EPOCHS, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0C1QanFaUOv",
        "outputId": "863d1f39-61cd-4e0e-c545-9fceba0d1d6b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "Splits done\n",
            "Standardization done\n",
            "(949960, 8, 102)\n",
            "Inputs done\n",
            "(949960, 32, 1)\n",
            "Outputs done\n",
            "(949960, 32, 1)\n",
            "Baseline done\n",
            "Epoch 1/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 3.8522e-04 - mean_absolute_error: 0.0052 - mean_squared_error: 2.0232e-04WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 79s 10ms/step - loss: 3.8505e-04 - mean_absolute_error: 0.0052 - mean_squared_error: 2.0227e-04\n",
            "Epoch 2/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 6.7736e-05 - mean_absolute_error: 0.0039 - mean_squared_error: 6.7259e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 6.7752e-05 - mean_absolute_error: 0.0039 - mean_squared_error: 6.7287e-05\n",
            "Epoch 3/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 5.8437e-05 - mean_absolute_error: 0.0036 - mean_squared_error: 5.7578e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 5.8439e-05 - mean_absolute_error: 0.0036 - mean_squared_error: 5.7585e-05\n",
            "Epoch 4/10\n",
            "7422/7422 [==============================] - ETA: 0s - loss: 5.2295e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.1063e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 5.2295e-05 - mean_absolute_error: 0.0034 - mean_squared_error: 5.1063e-05\n",
            "Epoch 5/10\n",
            "7418/7422 [============================>.] - ETA: 0s - loss: 5.0051e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 4.8779e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 5.0053e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 4.8783e-05\n",
            "Epoch 6/10\n",
            "7420/7422 [============================>.] - ETA: 0s - loss: 4.8567e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 4.7192e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 4.8567e-05 - mean_absolute_error: 0.0033 - mean_squared_error: 4.7193e-05\n",
            "Epoch 7/10\n",
            "7419/7422 [============================>.] - ETA: 0s - loss: 4.7806e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.6267e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 4.7806e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.6269e-05\n",
            "Epoch 8/10\n",
            "7417/7422 [============================>.] - ETA: 0s - loss: 4.7797e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.6284e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 4.7807e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.6298e-05\n",
            "Epoch 9/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 4.5063e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.3576e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 73s 10ms/step - loss: 4.5068e-05 - mean_absolute_error: 0.0032 - mean_squared_error: 4.3585e-05\n",
            "Epoch 10/10\n",
            "7421/7422 [============================>.] - ETA: 0s - loss: 4.5192e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.3598e-05WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mean_absolute_error,mean_squared_error\n",
            "7422/7422 [==============================] - 74s 10ms/step - loss: 4.5197e-05 - mean_absolute_error: 0.0031 - mean_squared_error: 4.3606e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /gdrive/MyDrive/playground/LSTM_ETHUSDT_12-12-2021/"
      ],
      "metadata": {
        "id": "4uJpiVxJajDP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = {}\n",
        "\n",
        "data['validation_score'] = history1.history['val_loss'][-1]\n",
        "data['unique_id'] = time.time()\n",
        "data['training_time_stamp'] = time.time()\n",
        "data['hyperparameters'] = {'batch_size': 128, 'lr': 1e-3, 'optimizer': 'Adam', 'n_layers': 3, 'n_units': 8, 'dropout': 0.2, 'weight_decay': 1e-4}\n",
        "data['symbol'] = 'ETHUSDT'\n",
        "data['x_mean'] = x_mean.values.tolist()\n",
        "data['x_std'] = x_std.values.tolist()\n",
        "data['y_mean'] = y_mean.values[0]\n",
        "data['y_std'] = y_std.values[0]\n",
        "\n",
        "with open('model_metrics_ETHUSDT.json', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ],
      "metadata": {
        "id": "MGF6YFv4SCN2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/gdrive/MyDrive/playground/LSTM_ETHUSDT_$(date +\"%d-%m-%Y\")"
      ],
      "metadata": {
        "id": "GtZ4P73wSqCu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/model_metrics_ETHUSDT.json /content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/"
      ],
      "metadata": {
        "id": "9OQMcp7tSqIP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save_weights('/content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/lstm_ETHUSDT')"
      ],
      "metadata": {
        "id": "0Izb1ctuSqKc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/ gs://crypto-forecasting-bucket/ETHUSDT/Model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlqL2DfrSqMi",
        "outputId": "3fd9fd94-2eb4-43c6-c277-132e37e78449"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/model_metrics_ETHUSDT.json [Content-Type=application/json]...\n",
            "/ [0/4 files][    0.0 B/427.6 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/lstm_ETHUSDT.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "/ [0/4 files][    0.0 B/427.6 KiB]   0% Done                                    \rCopying file:///content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/lstm_ETHUSDT.index [Content-Type=application/octet-stream]...\n",
            "Copying file:///content/gdrive/MyDrive/playground/LSTM_ETHUSDT_13-12-2021/checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [4/4 files][427.6 KiB/427.6 KiB] 100% Done                                    \n",
            "Operation completed over 4 objects/427.6 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE FOR REAL-TIME PREDICTIONS"
      ],
      "metadata": {
        "id": "vr50TRKj_GRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_folder = '../Data'\n",
        "file_name = 'btcusdt_exchange_full.csv'\n",
        "\n",
        "location_data = os.path.join(data_folder, file_name)\n",
        "\n",
        "exclude_x = ['NA', 'Open Time', 'Close Time', 'y', 'Date']\n",
        "\n",
        "val_split, test_split = None, None\n",
        "\n",
        "log_transformation_feat = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                           'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                           'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                           'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                           'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                           'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                           'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                           'rolling_avg_high_close_1h', 'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w',\n",
        "                           'rolling_avg_high_close_1m', 'rolling_max_high_close_1h', 'rolling_max_high_close_1d',\n",
        "                           'rolling_max_high_close_1w', 'rolling_max_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                           'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                           'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                           'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                           'rolling_avg_open_low_1d',\n",
        "                           'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                           'rolling_max_open_low_1d',\n",
        "                           'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                           'rolling_avg_spread_1d',\n",
        "                           'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                           'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                           'rolling_max_spread_1m', 'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d',\n",
        "                           'rolling_avg_Open Price_1w', 'rolling_avg_Open Price_1m',\n",
        "                           'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                           'rolling_max_Open Price_1m',\n",
        "                           'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                           'rolling_avg_High price_1m',\n",
        "                           'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                           'rolling_max_High price_1m',\n",
        "                           'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                           'rolling_avg_Low Price_1m',\n",
        "                           'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                           'rolling_max_Low Price_1m',\n",
        "                           'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                           'rolling_avg_Close Price_1m',\n",
        "                           'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                           'rolling_max_Close Price_1m']\n",
        "# features where we apply log transformation\n",
        "\n",
        "continuous_features = ['Open Price', 'High price', 'Low Price', 'Close Price', 'Volume Traded',\n",
        "                       'Quote asset Volume', 'Number of Trades', 'Taker buy base asset volume',\n",
        "                       'Taker buy quote asset volume', 'Mid Price', 'Open Price_prev', 'Close Price_prev',\n",
        "                       'Mid Price_prev', 'high_low', 'high_close', 'high_open', 'open_low', 'spread',\n",
        "                       'rolling_avg_high_low_1h', 'rolling_avg_high_low_1d', 'rolling_avg_high_low_1w',\n",
        "                       'rolling_avg_high_low_1m', 'rolling_max_high_low_1h',\n",
        "                       'rolling_max_high_low_1d', 'rolling_max_high_low_1w', 'rolling_max_high_low_1m',\n",
        "                       'rolling_min_high_low_1h', 'rolling_min_high_low_1d',\n",
        "                       'rolling_min_high_low_1w', 'rolling_min_high_low_1m', 'rolling_avg_high_close_1h',\n",
        "                       'rolling_avg_high_close_1d', 'rolling_avg_high_close_1w', 'rolling_avg_high_close_1m',\n",
        "                       'rolling_max_high_close_1h', 'rolling_max_high_close_1d', 'rolling_max_high_close_1w',\n",
        "                       'rolling_max_high_close_1m', 'rolling_min_high_close_1h', 'rolling_min_high_close_1d',\n",
        "                       'rolling_min_high_close_1w', 'rolling_min_high_close_1m', 'rolling_avg_high_open_1h',\n",
        "                       'rolling_avg_high_open_1d', 'rolling_avg_high_open_1w', 'rolling_avg_high_open_1m',\n",
        "                       'rolling_max_high_open_1h', 'rolling_max_high_open_1d',\n",
        "                       'rolling_max_high_open_1w', 'rolling_max_high_open_1m', 'rolling_min_high_open_1h',\n",
        "                       'rolling_min_high_open_1d',\n",
        "                       'rolling_min_high_open_1w', 'rolling_min_high_open_1m', 'rolling_avg_open_low_1h',\n",
        "                       'rolling_avg_open_low_1d',\n",
        "                       'rolling_avg_open_low_1w', 'rolling_avg_open_low_1m', 'rolling_max_open_low_1h',\n",
        "                       'rolling_max_open_low_1d',\n",
        "                       'rolling_max_open_low_1w', 'rolling_max_open_low_1m', 'rolling_min_open_low_1h',\n",
        "                       'rolling_min_open_low_1d',\n",
        "                       'rolling_min_open_low_1w', 'rolling_min_open_low_1m', 'rolling_avg_spread_1h',\n",
        "                       'rolling_avg_spread_1d',\n",
        "                       'rolling_avg_spread_1w', 'rolling_avg_spread_1m', 'rolling_max_spread_1h',\n",
        "                       'rolling_max_spread_1d', 'rolling_max_spread_1w',\n",
        "                       'rolling_max_spread_1m', 'rolling_min_spread_1h', 'rolling_min_spread_1d',\n",
        "                       'rolling_min_spread_1w', 'rolling_min_spread_1m',\n",
        "                       'rolling_avg_Open Price_1h', 'rolling_avg_Open Price_1d', 'rolling_avg_Open Price_1w',\n",
        "                       'rolling_avg_Open Price_1m',\n",
        "                       'rolling_max_Open Price_1h', 'rolling_max_Open Price_1d', 'rolling_max_Open Price_1w',\n",
        "                       'rolling_max_Open Price_1m',\n",
        "                       'rolling_min_Open Price_1h', 'rolling_min_Open Price_1d', 'rolling_min_Open Price_1w',\n",
        "                       'rolling_min_Open Price_1m',\n",
        "                       'rolling_avg_High price_1h', 'rolling_avg_High price_1d', 'rolling_avg_High price_1w',\n",
        "                       'rolling_avg_High price_1m',\n",
        "                       'rolling_max_High price_1h', 'rolling_max_High price_1d', 'rolling_max_High price_1w',\n",
        "                       'rolling_max_High price_1m',\n",
        "                       'rolling_min_High price_1h', 'rolling_min_High price_1d', 'rolling_min_High price_1w',\n",
        "                       'rolling_min_High price_1m',\n",
        "                       'rolling_avg_Low Price_1h', 'rolling_avg_Low Price_1d', 'rolling_avg_Low Price_1w',\n",
        "                       'rolling_avg_Low Price_1m',\n",
        "                       'rolling_max_Low Price_1h', 'rolling_max_Low Price_1d', 'rolling_max_Low Price_1w',\n",
        "                       'rolling_max_Low Price_1m',\n",
        "                       'rolling_min_Low Price_1h', 'rolling_min_Low Price_1d', 'rolling_min_Low Price_1w',\n",
        "                       'rolling_min_Low Price_1m',\n",
        "                       'rolling_avg_Close Price_1h', 'rolling_avg_Close Price_1d', 'rolling_avg_Close Price_1w',\n",
        "                       'rolling_avg_Close Price_1m',\n",
        "                       'rolling_max_Close Price_1h', 'rolling_max_Close Price_1d', 'rolling_max_Close Price_1w',\n",
        "                       'rolling_max_Close Price_1m',\n",
        "                       'rolling_min_Close Price_1h', 'rolling_min_Close Price_1d', 'rolling_min_Close Price_1w',\n",
        "                       'rolling_min_Close Price_1m']"
      ],
      "metadata": {
        "id": "SMmH_JelmDk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    # y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X, features_list\n",
        "\n",
        "def standardize(X, x_mean, x_std):\n",
        "    # x_mean = X.mean()\n",
        "    # x_std = X.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    print('Standardization done')\n",
        "    return X\n",
        "\n",
        "def generate_input_sequences(data, feats, seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-seq_len+1, seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(len(data)-seq_len+1):\n",
        "      x = np.expand_dims(dfvs[i:i+seq_len, :], axis = 0)\n",
        "      empty_np_array[i] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(X, batch_size):\n",
        "    X = X[-128:,:,:]\n",
        "    print(X.shape)\n",
        "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "    input_data = tf.data.Dataset.from_tensor_slices(X)\n",
        "    input_data = input_data.batch(batch_size)\n",
        "    input_data = input_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    return input_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    # display(df.head())\n",
        "    df = log_transformation(df)\n",
        "    # display(df.head())\n",
        "    X, features_list = _split(df)\n",
        "    X = standardize(X, x_mean, x_std)\n",
        "    X_seq = generate_input_sequences(X, X.columns.values.tolist(), 32)\n",
        "    print(X_seq.shape)\n",
        "    print('Inputs done')\n",
        "    input_data = generate_tf_data(X_seq, batch_size)\n",
        "    return input_data, features_list\n",
        "\n",
        "def create_model():\n",
        "    batch_size = 128\n",
        "    lr = 1e-3\n",
        "    optimizer = 'Adam'\n",
        "    n_layers = 3\n",
        "    n_units = 32\n",
        "    dropout = 0.2\n",
        "    weight_decay = 1e-4\n",
        "    if optimizer == 'Adam':\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    elif optimizer == 'SGD':\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "    lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "    if n_layers == 1:\n",
        "        lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "    else:\n",
        "        for n in range(n_layers):\n",
        "            if n == n_layers - 1:\n",
        "                lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "                #lstm_model.add(batchNormalization())\n",
        "                lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            else:\n",
        "                lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "                lstm_model.add(BatchNormalization())\n",
        "                lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "                \n",
        "    lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "    MAX_EPOCHS = 10\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "    lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    batch_size = 128\n",
        "\n",
        "    trial_lst_dict = [\n",
        "    {\n",
        "        \"id\": 1228,\n",
        "        \"name\": \"BTCUSDT\",\n",
        "        \"open_time\": 1502974920000,\n",
        "        \"open_price\": 4411,\n",
        "        \"high_price\": 4411,\n",
        "        \"low_price\": 4411,\n",
        "        \"close_price\": 4411,\n",
        "        \"volume_traded\": 0,\n",
        "        \"close_time\": 1502974979999,\n",
        "        \"quote_asset_volume\": 0,\n",
        "        \"number_of_trades\": 0,\n",
        "        \"taker_buy_base_asset_volume\": 0,\n",
        "        \"taker_buy_quote_asset_volume\": 0\n",
        "    },\n",
        "    {\n",
        "        \"id\": 163,\n",
        "        \"name\": \"BTCUSDT\",\n",
        "        \"open_time\": 1502942940000,\n",
        "        \"open_price\": 4261.48,\n",
        "        \"high_price\": 4261.48,\n",
        "        \"low_price\": 4261.48,\n",
        "        \"close_price\": 4261.48,\n",
        "        \"volume_traded\": 0,\n",
        "        \"close_time\": 1502942999999,\n",
        "        \"quote_asset_volume\": 0,\n",
        "        \"number_of_trades\": 0,\n",
        "        \"taker_buy_base_asset_volume\": 0,\n",
        "        \"taker_buy_quote_asset_volume\": 0\n",
        "    }\n",
        "    ]\n",
        "\n",
        "    trial_lst_dict = trial_lst_dict*25500\n",
        "\n",
        "    trial_df = pd.DataFrame(trial_lst_dict)\n",
        "    trial_df = trial_df.drop(['id', 'name'], axis = 1)\n",
        "    trial_df = trial_df.rename(columns = {'open_time': 'Open Time', 'open_price': 'Open Price', 'high_price': 'High price', 'low_price': 'Low Price', 'close_price': 'Close Price', 'volume_traded': 'Volume Traded', 'close_time': 'Close Time', 'quote_asset_volume': 'Quote asset Volume', 'number_of_trades': 'Number of Trades', 'taker_buy_base_asset_volume': 'Taker buy base asset volume', 'taker_buy_quote_asset_volume': 'Taker buy quote asset volume'})\n",
        "    input_data, features_list = preprocess_df(trial_df, batch_size)\n",
        "\n",
        "    model = create_model()\n",
        "    model.load_weights('/content/gdrive/MyDrive/playground/LSTM_BTCUSDT_12-12-2021/lstm_BTCUSDT')\n",
        "\n",
        "    predictions = model.predict(input_data)\n",
        "    to_return = predictions[-1]\n",
        "    "
      ],
      "metadata": {
        "id": "G_qsSCdumVfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    batch_size = 128\n",
        "    lr = 1e-3\n",
        "    optimizer = 'Adam'\n",
        "    n_layers = 3\n",
        "    n_units = 32\n",
        "    dropout = 0.2\n",
        "    weight_decay = 1e-4\n",
        "    if optimizer == 'Adam':\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    elif optimizer == 'SGD':\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps=500, decay_rate=0.9,staircase=True)  \n",
        "        opt = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "    lstm_model = tf.keras.models.Sequential()\n",
        "\n",
        "    if n_layers == 1:\n",
        "        lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "    else:\n",
        "        for n in range(n_layers):\n",
        "            if n == n_layers - 1:\n",
        "                lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=False))\n",
        "                #lstm_model.add(batchNormalization())\n",
        "                lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "            else:\n",
        "                lstm_model.add(tf.keras.layers.LSTM(n_units, return_sequences=True))\n",
        "                lstm_model.add(BatchNormalization())\n",
        "                lstm_model.add(tf.keras.layers.Dropout(dropout))\n",
        "                \n",
        "    lstm_model.add(tf.keras.layers.Dense(units=n_units, kernel_regularizer=regularizers.l2(l2=weight_decay)))\n",
        "\n",
        "    MAX_EPOCHS = 10\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='min')\n",
        "    lstm_model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=[tf.metrics.MeanAbsoluteError(), tf.keras.losses.MeanSquaredError()])\n",
        "    return lstm_model"
      ],
      "metadata": {
        "id": "1CB-FLA_xxZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.load_weights('/content/gdrive/MyDrive/playground/LSTM_BTCUSDT_12-12-2021/lstm_BTCUSDT')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCyOqA0idquG",
        "outputId": "fcda88fb-3208-4eee-8de2-b90173cad9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd24e13ea90>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial_lst_dict = [\n",
        "    {\n",
        "        \"id\": 1228,\n",
        "        \"name\": \"BTCUSDT\",\n",
        "        \"open_time\": 1502974920000,\n",
        "        \"open_price\": 4411,\n",
        "        \"high_price\": 4411,\n",
        "        \"low_price\": 4411,\n",
        "        \"close_price\": 4411,\n",
        "        \"volume_traded\": 0,\n",
        "        \"close_time\": 1502974979999,\n",
        "        \"quote_asset_volume\": 0,\n",
        "        \"number_of_trades\": 0,\n",
        "        \"taker_buy_base_asset_volume\": 0,\n",
        "        \"taker_buy_quote_asset_volume\": 0\n",
        "    },\n",
        "    {\n",
        "        \"id\": 163,\n",
        "        \"name\": \"BTCUSDT\",\n",
        "        \"open_time\": 1502942940000,\n",
        "        \"open_price\": 4261.48,\n",
        "        \"high_price\": 4261.48,\n",
        "        \"low_price\": 4261.48,\n",
        "        \"close_price\": 4261.48,\n",
        "        \"volume_traded\": 0,\n",
        "        \"close_time\": 1502942999999,\n",
        "        \"quote_asset_volume\": 0,\n",
        "        \"number_of_trades\": 0,\n",
        "        \"taker_buy_base_asset_volume\": 0,\n",
        "        \"taker_buy_quote_asset_volume\": 0\n",
        "    }\n",
        "    ]\n",
        "\n",
        "trial_lst_dict = trial_lst_dict*25500"
      ],
      "metadata": {
        "id": "WFIhGFOoccTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_df = pd.DataFrame(trial_lst_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "QVNBZQlucdoM",
        "outputId": "60ebe9d5-02af-4a77-f38d-0b8dbd2d0c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>open_time</th>\n",
              "      <th>open_price</th>\n",
              "      <th>high_price</th>\n",
              "      <th>low_price</th>\n",
              "      <th>close_price</th>\n",
              "      <th>volume_traded</th>\n",
              "      <th>close_time</th>\n",
              "      <th>quote_asset_volume</th>\n",
              "      <th>number_of_trades</th>\n",
              "      <th>taker_buy_base_asset_volume</th>\n",
              "      <th>taker_buy_quote_asset_volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50995</th>\n",
              "      <td>163</td>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>1502942940000</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>0</td>\n",
              "      <td>1502942999999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50996</th>\n",
              "      <td>1228</td>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>1502974920000</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1502974979999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50997</th>\n",
              "      <td>163</td>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>1502942940000</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>0</td>\n",
              "      <td>1502942999999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50998</th>\n",
              "      <td>1228</td>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>1502974920000</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>4411.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1502974979999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50999</th>\n",
              "      <td>163</td>\n",
              "      <td>BTCUSDT</td>\n",
              "      <td>1502942940000</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>4261.48</td>\n",
              "      <td>0</td>\n",
              "      <td>1502942999999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id     name  ...  taker_buy_base_asset_volume  taker_buy_quote_asset_volume\n",
              "50995   163  BTCUSDT  ...                            0                             0\n",
              "50996  1228  BTCUSDT  ...                            0                             0\n",
              "50997   163  BTCUSDT  ...                            0                             0\n",
              "50998  1228  BTCUSDT  ...                            0                             0\n",
              "50999   163  BTCUSDT  ...                            0                             0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial_df = trial_df.drop(['id', 'name'], axis = 1)"
      ],
      "metadata": {
        "id": "yjhHNSlXehj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_df = trial_df.rename(columns = {'open_time': 'Open Time', 'open_price': 'Open Price', 'high_price': 'High price', 'low_price': 'Low Price', 'close_price': 'Close Price', 'volume_traded': 'Volume Traded', 'close_time': 'Close Time', 'quote_asset_volume': 'Quote asset Volume', 'number_of_trades': 'Number of Trades', 'taker_buy_base_asset_volume': 'Taker buy base asset volume', 'taker_buy_quote_asset_volume': 'Taker buy quote asset volume'})"
      ],
      "metadata": {
        "id": "xYL8W64WdZer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from preprocessing_pipeline import preprocess_df\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#from parameters_modeling import exclude_x, log_transformation_feat, continuous_features, val_split, test_split, location_data\n",
        "#import wandb\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from wandb.keras import WandbCallback\n",
        "import tensorflow as tf\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# def select_last_year(df):\n",
        "#     df = df.sort_values('Open Time')\n",
        "#     df = df.iloc[:100000, :]\n",
        "#     df = df.reset_index(drop=True)\n",
        "#     print('Only Selecting the first 100000')\n",
        "#     return df\n",
        "\n",
        "def time_features_func(df):\n",
        "    df['Date'] = pd.to_datetime(df['Open Time'], unit = 'ms')\n",
        "    df['Year'] = pd.DatetimeIndex(df['Date']).year\n",
        "    df['Month'] = pd.DatetimeIndex(df['Date']).month\n",
        "    df['Day of Week'] = pd.DatetimeIndex(df['Date']).dayofweek\n",
        "    df['Day of Month'] = pd.DatetimeIndex(df['Date']).day\n",
        "    df['Day of Year'] = pd.DatetimeIndex(df['Date']).dayofyear\n",
        "    df['Week of Month'] = pd.DatetimeIndex(df['Date']).day // 7\n",
        "    df['Week of Year'] = df['Date'].dt.isocalendar().week.astype('int64')\n",
        "    df['hour'] =df.Date.dt.hour\n",
        "    df['minute'] =df.Date.dt.minute\n",
        "    df['minute_of_day'] = df.hour*60 + df.minute\n",
        "    print('Time preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_target_variables(df):\n",
        "    # df = df.drop('Unnamed: 0', axis=1)\n",
        "    df['Mid Price'] = (df['Open Price'] + df['Close Price'])/2\n",
        "    df['y'] = (df['Open Price'].shift(-1) + df['Close Price'].shift(-1))/2  # the target is predicting the next close price\n",
        "    df['benchmark'] = df['Mid Price']\n",
        "    print('Target preprocessing done')\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_last_reporting(df, columns=['Open Price', 'Close Price', 'Mid Price']):\n",
        "    \"\"\"Twice periods previously w.r.t the y variable\"\"\"\n",
        "    for column in columns:\n",
        "        df[column+'_prev'] = df[column].shift(1)\n",
        "    df = df.dropna()\n",
        "    print('Last preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_deviations(df):\n",
        "    df['high_low'] = df['High price'] - df['Low Price']\n",
        "    df['high_close'] = df['High price'] - df['Close Price']\n",
        "    df['high_open'] = df['High price'] - df['Open Price']\n",
        "    df['open_low'] = df['Close Price'] - df['Low Price']\n",
        "    df['spread'] =df['Close Price'] - df['Open Price']\n",
        "    df['spread_ind'] = 1*(df['spread'] < 0)\n",
        "    df['spread'] =np.abs(df.spread)\n",
        "    print('Deviations preprocessing done')\n",
        "    return df\n",
        "\n",
        "def create_stats(df, features = ['high_low', 'high_close', 'high_open', 'open_low', 'spread', 'Open Price', 'High price', 'Low Price', 'Close Price']):\n",
        "    for feature in features:\n",
        "        series_1h, series_1d, series_1w, series_1m =  df[feature].rolling(window = 60), df[feature].rolling(window = 1500), df[feature].rolling(window = 10000), df[feature].rolling(window = 50000)\n",
        "        df['rolling_avg_{}_1h'.format(feature)] = series_1d.mean() # rolling avg over 1 hour\n",
        "        df['rolling_avg_{}_1d'.format(feature)] = series_1d.mean() # rolling avg over 1 day\n",
        "        df['rolling_avg_{}_1w'.format(feature)] = series_1w.mean() # rolling avg over 1 week\n",
        "        df['rolling_avg_{}_1m'.format(feature)] = series_1m.mean() # rolling avg over 1 month\n",
        "        df['rolling_max_{}_1h'.format(feature)] = series_1d.max() # rolling max over 1 hour\n",
        "        df['rolling_max_{}_1d'.format(feature)] = series_1d.max() # rolling max over 1 day\n",
        "        df['rolling_max_{}_1w'.format(feature)] = series_1w.max() # rolling max over 1 week\n",
        "        df['rolling_max_{}_1m'.format(feature)] = series_1m.max()  # rolling max over 1 month\n",
        "        #df['rolling_min_{}_1h'.format(feature)] = series_1d.min() # rolling min over 1 hour\n",
        "        #df['rolling_min_{}_1d'.format(feature)] = series_1d.min() # rolling min over 1 day\n",
        "        #df['rolling_min_{}_1w'.format(feature)] = series_1w.min() # rolling min over 1 week\n",
        "        #df['rolling_min_{}_1m'.format(feature)] = series_1m.min() # rolling min over 1 month\n",
        "    df = df.dropna()\n",
        "    print('Rolling preprocessing done')\n",
        "    return df\n",
        "\n",
        "def log_transformation(df, log_transformation_features=log_transformation_feat):\n",
        "    for feature in log_transformation_features:\n",
        "        df['log_{}'.format(feature)] = np.log(1+df[feature].values)\n",
        "        df = df.drop(feature, axis=1)\n",
        "    print('Logs preprocessing done')\n",
        "    return df\n",
        "\n",
        "def _split(df, train_val_date=val_split, val_test_date=test_split, exclude_x=exclude_x):\n",
        "    # Standardize the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    features_list = [col for col in df.columns if col not in exclude_x]\n",
        "    X = df[features_list]\n",
        "    # y_train, y_val, y_test = df_train[['y']], df_val[['y']], df_test[['y']]\n",
        "    print('Splits done')\n",
        "    return X, features_list\n",
        "\n",
        "def standardize(X, x_mean, x_std):\n",
        "    # x_mean = X.mean()\n",
        "    # x_std = X.std()\n",
        "    X = (X - x_mean) / x_std\n",
        "    print('Standardization done')\n",
        "    return X\n",
        "\n",
        "def generate_input_sequences(data, feats, seq_len = 32):\n",
        "    empty_np_array = np.zeros((len(data)-seq_len+1, seq_len, len(feats)))\n",
        "    dfvs = data[feats].values\n",
        "    for i in range(len(data)-seq_len+1):\n",
        "      x = np.expand_dims(dfvs[i:i+seq_len, :], axis = 0)\n",
        "      empty_np_array[i] = x\n",
        "    return empty_np_array\n",
        "\n",
        "def generate_tf_data(X, batch_size):\n",
        "    X = X[-128:,:,:]\n",
        "    print(X.shape)\n",
        "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "    input_data = tf.data.Dataset.from_tensor_slices(X)\n",
        "    input_data = input_data.batch(batch_size)\n",
        "    input_data = input_data.prefetch(AUTOTUNE)\n",
        "\n",
        "    return input_data\n",
        "\n",
        "def preprocess_df(df, batch_size, location_data=location_data):\n",
        "    df = select_last_year(df)\n",
        "    df = time_features_func(df)\n",
        "    df = create_target_variables(df)\n",
        "    df = create_last_reporting(df)\n",
        "    df = create_deviations(df)\n",
        "    df = create_stats(df)\n",
        "    # display(df.head())\n",
        "    df = log_transformation(df)\n",
        "    # display(df.head())\n",
        "    X, features_list = _split(df)\n",
        "    #return X, features_list\n",
        "    X = standardize(X, x_mean, x_std)\n",
        "    X_seq = generate_input_sequences(X, X.columns.values.tolist(), 32)\n",
        "    print(X_seq.shape)\n",
        "    print('Inputs done')\n",
        "    input_data = generate_tf_data(X_seq, batch_size)\n",
        "    return input_data, features_list\n",
        "\n",
        "input_data, features_list = preprocess_df(trial_df, batch_size)"
      ],
      "metadata": {
        "id": "2jBHdGir_Vsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74dfc350-e205-47d9-996b-3ff571b3a619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only Selecting the last 2 years of data\n",
            "Time preprocessing done\n",
            "Target preprocessing done\n",
            "Last preprocessing done\n",
            "Deviations preprocessing done\n",
            "Rolling preprocessing done\n",
            "Logs preprocessing done\n",
            "Splits done\n",
            "Standardization done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(input_data)"
      ],
      "metadata": {
        "id": "pbpku_SJ22Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYX5tUcW6_En",
        "outputId": "4923803d-cb25-4203-811d-cabdf6fdf566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-9.8405553e-07, -5.4230331e-04, -4.8589339e-03, -4.5083687e-03,\n",
              "       -3.2698584e-03, -3.5638101e-03, -4.9586757e-03, -6.6472623e-03,\n",
              "       -8.5288947e-03, -1.0736486e-02, -1.2732004e-02, -1.4015911e-02,\n",
              "       -1.5037212e-02, -1.6451491e-02, -1.7964197e-02, -1.9437551e-02,\n",
              "       -2.0654945e-02, -2.1966469e-02, -2.3575917e-02, -2.5259724e-02,\n",
              "       -2.6925469e-02, -2.8193306e-02, -2.9496372e-02, -3.0684480e-02,\n",
              "       -3.1617083e-02, -3.2167535e-02, -3.2662116e-02, -3.3443663e-02,\n",
              "       -3.4581617e-02, -3.5566438e-02, -3.6236998e-02, -3.7210621e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF NOTEBOOK"
      ],
      "metadata": {
        "id": "WbC65FV2E0Cm"
      }
    }
  ]
}